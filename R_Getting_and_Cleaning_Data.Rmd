---
title: "R Notebook - Getting and Cleaning Data"
output: 
  html_notebook:
    toc: TRUE
    toc_float: TRUE
    theme: united
author: Jason Seto

---
This notebook was created to work through the examples and problems for getting and cleaning data

#Week
##Lesson
###Slide

---

#Week 1

##Reading XML

install.packages("XML")

```{r}
library(XML)
```

You can pull XML using the XML package in R. The code below accesses the w3schools site and pulls the breakfast node from the XML

1. Load the package
2. Give it a URL
3. Load it into R Memory
4. Rootnode -- the wrapper element for the entire document

```{r}
fileurl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileurl,useInternal=TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
```

This tells us that we get five different names in the rootnote (all related to food)
```{r}
names(rootNode)
```

###Directly access parts of the XML

If I want to pull the first food element

```{r}
rootNode[[1]]
```

Now if I want to keep drilling down, you can just subset by adding an additional element (in this case 1)

```{r}
rootNode[[1]][[1]]
```

###Programatically extract various parts of the xml

```{r tidy=TRUE} 
xmlSApply(rootNode,xmlValue)
```

###XPath

If you want to extract some of the XML document you can use the XPath language

Link: www.stat.berkeley.edu/~statcur/Workshop2/Presentations/XML.pdf

Some basic commands:

XPath                   Description
------------            ------------
/node                   Top Level Node
//node                  Node at any level
node[@attr_name]        Node with any attribute name
node[@attr-name='bob']  Node with attribute name attr-name = bob


###Get Items and menu prices

The xpathSApply command pulls from:

* rootnode - The whole XML
* //name   - which is the node with the "Name" header
* xmlvalue - the type of data you are pulling

```{r}
xpathSApply(rootNode,"//name", xmlValue)
```

Now try the same thing for price

```{r}
xpathSApply(rootNode,"//price", xmlValue)
```

###ESPN Page

http://www.espn.com/nfl/team/_/name/bal/baltimore-ravens

1. Right click on page
2. View Source
3. It's an extensive HTML

* Trying to pull scores against different teams

1. Load URL
2. Parse as HTML (note that it's slightly different from XML and you can see HTML at the top)
3. Use the xPathSApply to pull out 

```{r}
fileUrl <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl, useInternal = TRUE)
scores <- xpathSApply(doc, "//li[@class='score']", xmlValue)
teams <- xpathSApply(doc, "//li[@class='team-name']", xmlValue)

scores
teams
```


```{r}
a <- "http://sports.yahoo.com/nfl/scoreboard/"
b <- htmlTreeParse(fileURL, useInternalNodes =TRUE)
scores <- xpathSApply(doc, "//li[@id='scoreboard-group-2']",class="score")
scores
```

//*[@id="scoreboard-group-2"]/div/ul/li[1]/div/a/div/div[2]/div[1]/ul/li[2]/div[2]/div/span[1]

## Reading in JSON

Javascript Object Notation

* Lightweight data storage language
* Data stored as numbers, strings, booleans, and objects

https://api.github.com/users/jtleek/repos


### JsonLite

1. Install packages needed
*install.packages("jsonlite")
*install.packages("curl")

2. These certificate installations were for ELVT, but they work fine off of the network
*install.packages("httr")
*library(httr)
*set_config(config(ssl_verifypeer = 0L))

3. Call the library
*library(jsonlite)

### Nested objects in JSON

* https://api.github.com/users/jtleek/repos
* You can visit the repo and see that we can call the JSON file jsonData
```{r}
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
names(jsonData)
```

### Nested objects in JSON

* This pulls all the names
```{r}
names(jsonData$owner)
```
* This pulls specific portions of the jsonData for the owner login field
```{r}
jsonData$owner$login
```
### Writing data frames to JSON
* pretty = true gives you nice indentation for reading
* cat commands prints out what you need
```{r}
myjson <- toJSON(iris,pretty=TRUE)
cat(myjson)
```

### Convert JSON to table
```{r}
iris2 <- fromJSON(myjson)
head(iris2)
```
### Additional resources
*json.org
*r-bloggers.com/new-package-jsonlite-a-smarter-json-encoderdecoder
*jsonlite vignette

## The data.table Package
* Inherets from data frame
* all functions that accept data.frame work on data.table work much faster

### Create data tables

* You can create tables exactly like a data frame
install.packages('data.table')

```{r}
library(data.table)
```

```{r}
DF = data.frame(x=rnorm(9), y=rep(c("a","b","c"), each=3),z=rnorm(9))
head(DF,3)
```

* We can create a data frame the exact same way 
```{r}
DT = data.table(x=rnorm(9), y=rep(c("a","b","c"), each=3),z=rnorm(9))
head(DT,3)
```
### View tables in memory
```{r}
tables()
```
* This tells us the name of the data table, how many rows, and how many megabytes, and if there is a key

### Subsetting Rows
```{r}
DT[2,]
```

* This pulls out just the second row of the data table

```{r}
DT[DT$y=="a"]
```
* You can also subset by specific values. Here we are subsetting all the values where column y = "a"


#Week 1 Quiz

## 1
1. 
The American Community Survey distributes downloadable data about United States communities. Download the 2006 microdata survey about housing for the state of Idaho using download.file() from here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv

and load the data into R. The code book, describing the variable names is here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf

How many properties are worth $1,000,000 or more?

31

53

24

2076
```{r}
?download.file
```

```{r}
setwd("~/GitHub/Kelburn_Repository")
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(url=fileUrl,destfile="idaho_housing.csv")
list.files(".")
```

```{r}
?read.csv()
?data.frame()
install.packages("sqldf")
library("sqldf")

idaho_housing<- read.csv("idaho_housing.csv", header=T)

head(idaho_housing)

val_Bucket_Counts <- sqldf ("

select 
  VAL
  ,count(*) as count
from idaho_housing
  where val >= 24
  group by VAL

                       ")

val_Bucket_Counts

```


## 2 

Use the data you loaded from Question 1. Consider the variable FES in the code book. Which of the "tidy data" principles does this variable violate?

Numeric values in tidy data can not represent categories.

Tidy data has no missing values.

X Tidy data has one variable per column.

Each variable in a tidy data set has been transformed to be interpretable.

```{r}
str(idaho_housing$FES)
```
Family type and employment status

```{r}
FES_Bucket_Count <- sqldf("
                          select fes
                          , count(*) as count
                        from 
                          idaho_housing
                          group by FES")
```

## 3
Download the Excel spreadsheet on Natural Gas Aquisition Program here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx

Read rows 18-23 and columns 7-15 into R and assign the result to a variable called:

dat
What is the value of:

sum(dat$Zip*dat$Ext,na.rm=T)
(original data source: http://catalog.data.gov/dataset/natural-gas-acquisition-program)

154339

36534720

NA

0
```{r}
install.packages("xlsx")
library(xlsx)
install.packages("rJava")
library("rJava")
```

```{r}
setwd("~/Github/Kelburn_Repository")
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(url=fileURL,destfile="NGAP.xlsx")
```
```{r}
NGAP<-read.xlsx("~/Github/Kelburn_Repository/NGAP.xlsx",header=true)
```


