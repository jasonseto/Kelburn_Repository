---
title: "R Notebook - Getting and Cleaning Data"
date: July 2017
output: 
  html_notebook:
    toc: TRUE
    toc_float: TRUE
    theme: united
author: Jason Seto

---
This notebook was created to work through the examples and problems for getting and cleaning data

#Week
##Lesson
###Slide

---

#Week 1

##Reading XML

install.packages("XML")

```{r}
library(XML)
```

You can pull XML using the XML package in R. The code below accesses the w3schools site and pulls the breakfast node from the XML

1. Load the package
2. Give it a URL
3. Load it into R Memory
4. Rootnode -- the wrapper element for the entire document

```{r}
fileurl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileurl,useInternal=TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
```

This tells us that we get five different names in the rootnote (all related to food)
```{r}
names(rootNode)
```

###Directly access parts of the XML

If I want to pull the first food element

```{r}
rootNode[[1]]
```

Now if I want to keep drilling down, you can just subset by adding an additional element (in this case 1)

```{r}
rootNode[[1]][[1]]
```

###Programatically extract various parts of the xml
```{r tidy=TRUE} 
xmlSApply(rootNode,xmlValue)
```

###XPath

If you want to extract some of the XML document you can use the XPath language

Link: www.stat.berkeley.edu/~statcur/Workshop2/Presentations/XML.pdf

####Some basic commands:

XPath                   Description
------------            ------------
/node                   Top Level Node
//node                  Node at any level
node[@attr_name]        Node with any attribute name
node[@attr-name='bob']  Node with attribute name attr-name = bob

###Get Items and menu prices

The xpathSApply command pulls from:

* rootnode - The whole XML
* //name   - which is the node with the "Name" header
* xmlvalue - the type of data you are pulling

```{r}
xpathSApply(rootNode,"//name", xmlValue)
```

Now try the same thing for price
```{r}
xpathSApply(rootNode,"//price", xmlValue)
```

###ESPN Page

http://www.espn.com/nfl/team/_/name/bal/baltimore-ravens

1. Right click on page
2. View Source
3. It's an extensive HTML

Trying to pull scores against different teams

1. Load URL
2. Parse as HTML (note that it's slightly different from XML and you can see HTML at the top)
3. Use the xPathSApply to pull out 

```{r}
fileUrl <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl, useInternal = TRUE)
scores <- xpathSApply(doc, "//li[@class='score']", xmlValue)
teams <- xpathSApply(doc, "//li[@class='team-name']", xmlValue)

scores
teams
```


```{r}
a <- "http://sports.yahoo.com/nfl/scoreboard/"
b <- htmlTreeParse(fileURL, useInternalNodes =TRUE)
scores <- xpathSApply(doc, "//li[@id='scoreboard-group-2']",class="score")
scores
```

//*[@id="scoreboard-group-2"]/div/ul/li[1]/div/a/div/div[2]/div[1]/ul/li[2]/div[2]/div/span[1]

## Reading in JSON

Javascript Object Notation

* Lightweight data storage language
* Data stored as numbers, strings, booleans, and objects

https://api.github.com/users/jtleek/repos


### JsonLite

1. Install packages needed
*install.packages("jsonlite")
*install.packages("curl")

2. These certificate installations were for ELVT, but they work fine off of the network
*install.packages("httr")
*library(httr)
*set_config(config(ssl_verifypeer = 0L))

3. Call the library
*library(jsonlite)

### Nested objects in JSON

* https://api.github.com/users/jtleek/repos
* You can visit the repo and see that we can call the JSON file jsonData
```{r}
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
names(jsonData)
```

### Nested objects in JSON

* This pulls all the names
```{r}
names(jsonData$owner)
```
* This pulls specific portions of the jsonData for the owner login field
```{r}
jsonData$owner$login
```
### Writing data frames to JSON
* pretty = true gives you nice indentation for reading
* cat commands prints out what you need
```{r}
myjson <- toJSON(iris,pretty=TRUE)
cat(myjson)
```

### Convert JSON to table
```{r}
iris2 <- fromJSON(myjson)
head(iris2)
```
### Additional resources
* json.org
* r-bloggers.com/new-package-jsonlite-a-smarter-json-encoderdecoder
* jsonlite vignette

## The data.table Package
* Inherets from data frame
* all functions that accept data.frame work on data.table work much faster

### Create data tables

* You can create tables exactly like a data frame

install.packages('data.table')
```{r}
library(data.table)
```

```{r}
DF = data.frame(x=rnorm(9), y=rep(c("a","b","c"), each=3),z=rnorm(9))
head(DF,3)
```

* We can create a data frame the exact same way 
```{r}
DT = data.table(x=rnorm(9), y=rep(c("a","b","c"), each=3),z=rnorm(9))
head(DT,3)
```
### View tables in memory
```{r}
tables()
```
* This tells us the name of the data table, how many rows, and how many megabytes, and if there is a key

### Subsetting Rows
```{r}
DT[2,]
```

* This pulls out just the second row of the data table

```{r}
DT[DT$y=="a"]
```
* You can also subset by specific values. Here we are subsetting all the values where column y = "a"


#Week 1 Quiz

## 1
1. 
The American Community Survey distributes downloadable data about United States communities. Download the 2006 microdata survey about housing for the state of Idaho using download.file() from here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv

and load the data into R. The code book, describing the variable names is here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf

How many properties are worth $1,000,000 or more?

31

53

24

2076
```{r}
?download.file
```

```{r}
setwd("~/GitHub/Kelburn_Repository")
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(url=fileUrl,destfile="idaho_housing.csv")
list.files(".")
```

```{r}
?read.csv()
?data.frame()
install.packages("sqldf")
library("sqldf")

idaho_housing<- read.csv("idaho_housing.csv", header=T)

head(idaho_housing)

val_Bucket_Counts <- sqldf ("

select 
  VAL
  ,count(*) as count
from idaho_housing
  group by VAL

                       ")

val_Bucket_Counts

```


## 2 

Use the data you loaded from Question 1. Consider the variable FES in the code book. Which of the "tidy data" principles does this variable violate?

Numeric values in tidy data can not represent categories.

Tidy data has no missing values.

X Tidy data has one variable per column.

Each variable in a tidy data set has been transformed to be interpretable.

```{r}
str(idaho_housing$FES)
```
Family type and employment status

```{r}
FES_Bucket_Count <- sqldf("
                          select fes
                          , count(*) as count
                        from 
                          idaho_housing
                          group by FES")

FES_Bucket_Count
```

## 3
Download the Excel spreadsheet on Natural Gas Aquisition Program here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx

Read rows 18-23 and columns 7-15 into R and assign the result to a variable called:

dat
What is the value of:

sum(dat$Zip*dat$Ext,na.rm=T)
(original data source: http://catalog.data.gov/dataset/natural-gas-acquisition-program)

154339

36534720

NA

0


###Install the right packages

* Initially tried to use the XLSX package, but I did not have JAVA available, so had to drop that
```{r}
install.packages("xlsx")
library(xlsx)
install.packages("rJava")
library("rJava")
```
* Tried the Excel package but it was not available for my version
```{r}
install.packages("excel")
library("excel")

```
####Open XLSX was the way to go
```{r}
install.packages("openxlsx")
library("openxlsx")
```


Something wrong with the file download, and ended up having to manually download it and transfer to the github folder. Must be a token issue

```{r}
setwd("~/Github/Kelburn_Repository")
fileURL <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xls"
download.file(url=fileURL,destfile="Test.xls")
```

###Filter for rows and columns in the question
```{r}
dat<- read.xlsx("NGAP.xlsx", sheet = 1, startRow = 1, colNames = TRUE,
rowNames = FALSE, detectDates = FALSE, skipEmptyRows = TRUE,
skipEmptyCols = TRUE, rows = 18:23, cols = 7:15, check.names = FALSE,
namedRegion = NULL, na.strings = "NA", fillMergedCells = FALSE)

head(dat)
```

```{r}
sum(dat$Zip*dat$Ext,na.rm=T)
```
##4

Read the XML data on Baltimore restaurants from here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml

How many restaurants have zipcode 21231?

181

100

127

17

### Install proper packages
install.packages("XML")
library("XML")
library("sqldf")
```{r}
fileurl <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileurl,useInternal=TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
```

```{r}
zip <- xpathSApply(rootNode,"//zipcode", xmlValue)

zip_data <- data.frame(zip)

query_zip <- sqldf("
                   select count(*) as count_zips
                   from zip_data
                    where zip in ('21231')
                   ")

query_zip
```

##5
The American Community Survey distributes downloadable data about United States communities. Download the 2006 microdata survey about housing for the state of Idaho using download.file() from here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv

using the fread() command load the data into an R object

DT
The following are ways to calculate the average value of the variable

pwgtp15
broken down by sex. Using the data.table package, which will deliver the fastest user time?




```{r}
#https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv

URL<- https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv
  
download.file("http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv","ACS.csv")
```

```{r}

library(data.table)

fread(input, sep="auto", sep2="auto", nrows=-1L, header="auto", na.strings="NA", file,
stringsAsFactors=FALSE, verbose=getOption("datatable.verbose"), autostart=1L,
skip=0L, select=NULL, drop=NULL, colClasses=NULL,
integer64=getOption("datatable.integer64"),         # default: "integer64"
dec=if (sep!=".") "." else ",", col.names, 
check.names=FALSE, encoding="unknown", quote="\"", 
strip.white=TRUE, fill=FALSE, blank.lines.skip=FALSE, key=NULL, 
showProgress=getOption("datatable.showProgress"),   # default: TRUE
data.table=getOption("datatable.fread.datatable")   # default: TRUE
)

DT<- fread("ACS.csv")

head(DT)
```

mean(DT$pwgtp15,by=DT$SEX)

tapply(DT$pwgtp15,DT$SEX,mean)

mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)

DT[,mean(pwgtp15),by=SEX]

rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]

sapply(split(DT$pwgtp15,DT$SEX),mean)

### Start the clock!
ptm <- proc.time()
mean(DT$pwgtp15,by=DT$SEX)
### Stop the clock
proc.time() - ptm

### Start the clock!
ptm <- proc.time()
tapply(DT$pwgtp15,DT$SEX,mean)
### Stop the clock
proc.time() - ptm

### Start the clock!
ptm <- proc.time()
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
### Stop the clock
proc.time() - ptm

### Start the clock!
ptm <- proc.time()
DT[,mean(pwgtp15),by=SEX]
### Stop the clock
proc.time() - ptm

### Start the clock!
ptm <- proc.time()
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
### Stop the clock
proc.time() - ptm

### Start the clock!
ptm <- proc.time()
sapply(split(DT$pwgtp15,DT$SEX),mean)
### Stop the clock
proc.time() - ptm

#Week 2

## Reading from MySQL
### mySQL
1. Free and widely used open source
2. Structured in DB, tables fields
3. Each row is a record

mysql.com

### Example structure

1. You can have multiple schemas that link in a star schema

### Install MySql
http://dev.mysql.com/doc/refman/5.7/en/installing.html

install.packages("RMySQL")


```{r}

```

1. You can install it for a variety of operating systems
2. Creating a mysql database on your own system

### Install RMySQL

1. On Windows: vanderbilt.edu
2. Official instructions: biostat.mc.vanderbilt.edu
3. ahschuls.de/2013/07...






## Reading from HDFS
## Reading from the Web
## Reading from API
## Reading from other sources

